{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7033f181-f555-4e81-a0a5-76e95a7e414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatahap27/.conda/envs/NER-Project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from NERModel import NERMOEE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import fasttext\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f78e133-4480-43e9-9628-b1172ba4b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb76fb1a-00f1-48af-aec0-ef6bc944c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    sd_dataset = nergritImport()\n",
    "    td_dataset = helpdeskUBImport()\n",
    "    \n",
    "    sd_dataset = preprocessNergrit(sd_dataset)\n",
    "    \n",
    "    if 'ner_tags' not in td_dataset.columns:\n",
    "        td_dataset = preprocessHelpdesk(td_dataset)\n",
    "    \n",
    "    return sd_dataset, td_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c6b3e7-722a-4e43-9aed-26a47aad76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nergritImport():\n",
    "    \n",
    "    # Import dataset Nergrit NER dari Huggingface\n",
    "    dataset = load_dataset(\"id_nergrit_corpus\", 'ner')\n",
    "    \n",
    "    # Gabung data train, test, dan validation\n",
    "    test = dataset['test']\n",
    "    train = dataset['train']\n",
    "    validation = dataset['validation']\n",
    "\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    id_data = []\n",
    "\n",
    "    for dataset in [train, test, validation]:\n",
    "        tokens.extend(dataset['tokens'])\n",
    "        ner_tags.extend(dataset['ner_tags'])\n",
    "        id_data.extend(dataset['id'])\n",
    "\n",
    "    dataset = {\"id\":id_data, \"tokens\":tokens, \"ner_tags\":ner_tags}\n",
    "    \n",
    "    # Hapus kolom id\n",
    "    dataset.pop('id')\n",
    "    \n",
    "    # Konversi dataset ke tipe data dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c552a4e-ec6d-4a57-ac5a-45ffa06db46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helpdeskUBImport():\n",
    "    \n",
    "    try:\n",
    "        with open('target_domain_preprocessed.json', 'r') as file:\n",
    "            dataset = json.load(file)\n",
    "        \n",
    "        dataset = pd.DataFrame(dataset)\n",
    "        \n",
    "    except:\n",
    "        # Download dataset helpdesk TIK UB\n",
    "        url = 'https://docs.google.com/spreadsheets/d/1PzUlTZwY6IySZ7VNotDIH3h9hnuRIuy40CgAS7BcFkE/export?gid=1874021283&format=csv'\n",
    "        output_file = 'test.csv'\n",
    "\n",
    "        urllib.request.urlretrieve(url, output_file)\n",
    "\n",
    "        # Import file csv dan convert ke dataframe\n",
    "        dataset = pd.read_csv('test.csv', usecols=['body'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c753ad-93aa-4e83-8711-ed28b568fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessNergrit(sd_dataset:pd.DataFrame):\n",
    "    dataset = pd.DataFrame()\n",
    "    \n",
    "    # Hapus data duplikat dataset\n",
    "    dataset['tokens'] = sd_dataset['tokens'].apply(tuple)\n",
    "    dataset['ner_tags'] = sd_dataset['ner_tags'].apply(tuple)\n",
    "\n",
    "    dataset = dataset.drop_duplicates()\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=11)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    test_dataset = test_dataset.reset_index(drop=True)\n",
    "    \n",
    "    # Gabung train test dataset menjadi dictionary\n",
    "    dataset = {\"train_dataset\": train_dataset, \"test_dataset\": test_dataset}\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def remove_blockquote_tag(sentence):\n",
    "    if sentence is not None:\n",
    "        return re.sub(r'<blockquote>.*?</blockquote>', '', sentence, flags=re.DOTALL)\n",
    "    return None\n",
    "\n",
    "def remove_html_tag(sentence):\n",
    "    if sentence is not None:\n",
    "        return re.sub(r'<[^>]+>', '', sentence)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df789e33-4a3a-41bc-b9e4-a794e52be2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessHelpdesk(td_dataset:pd.DataFrame):\n",
    "    dataset = pd.DataFrame()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bryanahusna/my-nergrit-model\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"bryanahusna/my-nergrit-model\").to(device)\n",
    "    \n",
    "    # Hapus data duplikat dataset\n",
    "    dataset['body'] = td_dataset['body']\n",
    "    dataset = dataset.drop_duplicates(subset='body')\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "    \n",
    "    # Hapus data yang memiliki kata lebih dari 200\n",
    "    dataset['word_count'] = dataset['body'].apply(count_words)\n",
    "    dataset = dataset[dataset['word_count'] <= 200]\n",
    "    dataset = dataset.drop(columns=['word_count'])\n",
    "\n",
    "    # Hapus tag blockquote dan isinyya\n",
    "    dataset['body'] = dataset['body'].apply(remove_blockquote_tag)\n",
    "    \n",
    "    # Hapus tag html\n",
    "    dataset['body'] = dataset['body'].apply(remove_html_tag)\n",
    "    \n",
    "    dataset_arr = np.array(dataset['body']).tolist()\n",
    "    \n",
    "    # Tokenisasi data\n",
    "    tokens_vector = tokenizer(dataset_arr, padding=True, return_tensors='pt').to(device)\n",
    "    attention_mask = tokens_vector['attention_mask']\n",
    "\n",
    "    tokens = []\n",
    "    \n",
    "    for d in dataset_arr: \n",
    "        tokens.append(tokenizer.tokenize(d))\n",
    "    \n",
    "    # Predict\n",
    "    batch_size = 16\n",
    "    tokens_vector = torch.utils.data.DataLoader(tokens_vector['input_ids'], batch_size=batch_size)\n",
    "    attention_mask = torch.utils.data.DataLoader(attention_mask, batch_size=batch_size)\n",
    "    \n",
    "    ner_tags = []\n",
    "    \n",
    "    for t, a in zip(tokens_vector, attention_mask):\n",
    "        output = model(t, a)\n",
    "        logits = output.logits\n",
    "\n",
    "        predicted = torch.argmax(logits, dim=2)\n",
    "        predicted = predicted.masked_fill(a==0, -1)\n",
    "        predicted_list = predicted.tolist()\n",
    "        \n",
    "        result_list = [[value for value in sublist if value != -1] for sublist in predicted_list]\n",
    "        result_list = [sublist[1:-1] for sublist in result_list]\n",
    "        \n",
    "        ner_tags.extend(result_list)\n",
    "    \n",
    "    result_dict = {\"tokens\":tokens, \"ner_tags\":ner_tags}\n",
    "    result = pd.DataFrame(result_dict)\n",
    "    \n",
    "    with open('target_domain_preprocessed.json', 'w') as file:\n",
    "            json.dump(result_dict, file,)\n",
    "                          \n",
    "    return result\n",
    "    \n",
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869b81aa-735e-4ace-9ca1-7b7f827a72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_dataset, td_dataset = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "041bfe52-63d0-4e0b-ad97-cbe0bfcea264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[sudah, dicoba, dan, tet, ##ep, gak, bisa, mba...</td>\n",
       "      <td>[38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 6, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bisa, di, screen, ##sh, ##ot, tampilan, untuk...</td>\n",
       "      <td>[38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[y, ##th, ., bapak, /, ibu, ##nam, ##a, saya, ...</td>\n",
       "      <td>[38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 12, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[y, ##th, ., kepala, sti, ub, mohon, bantuan, ...</td>\n",
       "      <td>[38, 38, 38, 11, 30, 30, 38, 38, 38, 38, 38, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[terima, kasih, atas, informasi, yang, anda, b...</td>\n",
       "      <td>[38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>[jadi, awalnya, akun, gapura, saya, ke, log, o...</td>\n",
       "      <td>[38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9251</th>\n",
       "      <td>[nama, :, ilham, ramadan, gunawan, nim, :, 175...</td>\n",
       "      <td>[38, 38, 12, 31, 31, 38, 38, 17, 36, 36, 36, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>[pada, saat, login, email, ub, ,, kenapa, ada,...</td>\n",
       "      <td>[38, 38, 38, 38, 11, 38, 38, 38, 38, 38, 38, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>[mohon, bantuan, res, ##et, password, akun, ba...</td>\n",
       "      <td>[38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>[perken, ##alkan, saya, sal, ##ma, arie, ##n, ...</td>\n",
       "      <td>[38, 38, 38, 12, 31, 31, 31, 31, 31, 31, 31, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9255 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0     [sudah, dicoba, dan, tet, ##ep, gak, bisa, mba...   \n",
       "1     [bisa, di, screen, ##sh, ##ot, tampilan, untuk...   \n",
       "2     [y, ##th, ., bapak, /, ibu, ##nam, ##a, saya, ...   \n",
       "3     [y, ##th, ., kepala, sti, ub, mohon, bantuan, ...   \n",
       "4     [terima, kasih, atas, informasi, yang, anda, b...   \n",
       "...                                                 ...   \n",
       "9250  [jadi, awalnya, akun, gapura, saya, ke, log, o...   \n",
       "9251  [nama, :, ilham, ramadan, gunawan, nim, :, 175...   \n",
       "9252  [pada, saat, login, email, ub, ,, kenapa, ada,...   \n",
       "9253  [mohon, bantuan, res, ##et, password, akun, ba...   \n",
       "9254  [perken, ##alkan, saya, sal, ##ma, arie, ##n, ...   \n",
       "\n",
       "                                               ner_tags  \n",
       "0     [38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 6, 25...  \n",
       "1      [38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38]  \n",
       "2     [38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 12, 3...  \n",
       "3     [38, 38, 38, 11, 30, 30, 38, 38, 38, 38, 38, 3...  \n",
       "4     [38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 3...  \n",
       "...                                                 ...  \n",
       "9250  [38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 3...  \n",
       "9251  [38, 38, 12, 31, 31, 38, 38, 17, 36, 36, 36, 3...  \n",
       "9252  [38, 38, 38, 38, 11, 38, 38, 38, 38, 38, 38, 3...  \n",
       "9253  [38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 1...  \n",
       "9254  [38, 38, 38, 12, 31, 31, 31, 31, 31, 31, 31, 3...  \n",
       "\n",
       "[9255 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9780ab9f-e98c-46d0-b94a-c4c62ce8cc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9255, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d2e768b-8847-4522-95f7-62cf5f3e6ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_dataset['test_dataset'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34577618-d7a5-4026-a07c-04899e3d1f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13462, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_dataset['train_dataset'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f54a7-4417-4ac6-987a-b5f8ab30a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelConfig():\n",
    "    try:\n",
    "        with open('config.json', 'r') as file:\n",
    "            config_dict = json.load(file)\n",
    "\n",
    "        if 'dropout_rate' not in config_dict or config_dict['dropout_rate'] is None:\n",
    "            print('input dropout_rate:')\n",
    "            dropout_rate = input()\n",
    "            config_dict['dropout_rate']=dropout_rate\n",
    "            \n",
    "            with open('config.json', 'w') as file:\n",
    "                json.dump(config_dict, file)\n",
    "            \n",
    "        if 'embedding_size' not in config_dict or config_dict['embedding_size'] is None:\n",
    "            print('input embedding_size:')def getModel():\n",
    "    config = modelConfig()\n",
    "    if 'model' in config and config['model'] is not None:\n",
    "        print('return model')\n",
    "        return\n",
    "        \n",
    "    print('create model')\n",
    "            embedding_size = input()\n",
    "            config_dict['embedding_size']=embedding_size\n",
    "            \n",
    "            with open('config.json', 'w') as file:\n",
    "                json.dump(config_dict, file)\n",
    "            \n",
    "        if 'hidden_state' not in config_dict or config_dict['hidden_state'] is None:\n",
    "            print('input hidden_state:')\n",
    "            hidden_state = input()\n",
    "            config_dict['hidden_state']=hidden_state\n",
    "            \n",
    "            with open('config.json', 'w') as file:\n",
    "                json.dump(config_dict, file)\n",
    "            \n",
    "        if 'tag' not in config_dict or config_dict['tag'] is None:\n",
    "            print('input tag:')\n",
    "            tag = input()\n",
    "            config_dict['tag']=tag\n",
    "            \n",
    "            with open('config.json', 'w') as file:\n",
    "                json.dump(config_dict, file)\n",
    "                \n",
    "        with open('config.json', 'r') as file:\n",
    "            config_dict = json.load(file)\n",
    "    except:\n",
    "        config_dict = {}\n",
    "        \n",
    "        print('input dropout_rate:')\n",
    "        dropout_rate = input()\n",
    "        config_dict['dropout_rate']=dropout_rate\n",
    "        \n",
    "        print('input embedding_size:')\n",
    "        embedding_size = input()\n",
    "        config_dict['embedding_size']=embedding_size\n",
    "            \n",
    "        print('input hidden_state:')\n",
    "        hidden_state = input()\n",
    "        config_dict['hidden_state']=hidden_state\n",
    "            \n",
    "        print('input tag:')\n",
    "        tag = input()\n",
    "        config_dict['tag']=tag\n",
    "        \n",
    "        with open('config.json', 'w') as file:\n",
    "            json.dump(config_dict, file,)\n",
    "    \n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8671d1f-2c8c-4d15-a7b6-aa3c1d6148f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    config = modelConfig()\n",
    "    if 'model' in config and config['model'] is not None:\n",
    "        print('return model')\n",
    "        return\n",
    "        \n",
    "    model = NERMOEE(256, 512,)\n",
    "    print('create model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel1",
   "language": "python",
   "name": "kernel1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
