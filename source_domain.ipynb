{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d13791b-6efd-423d-abb3-396d7aca38d0",
   "metadata": {},
   "source": [
    "# Dataset Load: NERGRIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5247e2b9-9e84-4b50-8afb-9617be69f40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatahap27/.conda/envs/NER-Project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc5edcc-b14a-452a-8838-dd2c70d9e9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 12532\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2399\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2521\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nergrit_dataset = load_dataset(\"id_nergrit_corpus\", 'ner')\n",
    "nergrit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b581c2e-741f-4972-954b-dfc1f8198d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nergrit_dataset['test']\n",
    "train = nergrit_dataset['train']\n",
    "validation = nergrit_dataset['validation']\n",
    "\n",
    "tokens = []\n",
    "ner_tags = []\n",
    "id_data = []\n",
    "\n",
    "for dataset in [train, test, validation]:\n",
    "    tokens.extend(dataset['tokens'])\n",
    "    ner_tags.extend(dataset['ner_tags'])\n",
    "    id_data.extend(dataset['id'])\n",
    "\n",
    "nergrit_tokens_combined = tokens.copy()\n",
    "\n",
    "dataset = {\"id\":id_data, \"tokens\":tokens, \"ner_tags\":ner_tags}\n",
    "\n",
    "dataset.pop('id')\n",
    "    \n",
    "dataset = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a58c68-78c9-4044-a122-050f6f5e1a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Indonesia, mengekspor, produk, industri, skal...</td>\n",
       "      <td>(4, 38, 38, 38, 38, 38, 38, 4, 23, 38, 38, 38,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Martahan, Sohuturon, ,, CNN, Indonesia, |, Ra...</td>\n",
       "      <td>(12, 31, 38, 11, 30, 38, 1, 20, 20, 20, 20, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Polri, Awasi, Peredaran, Bahan, ', The, Mothe...</td>\n",
       "      <td>(9, 38, 38, 38, 38, 14, 33, 33, 33, 38, 9, 38,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Jakarta, ,, CNN, Indonesia, --, Kepolisian, N...</td>\n",
       "      <td>(4, 38, 11, 30, 38, 9, 28, 28, 28, 38, 9, 38, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Kepala, Divisi, Hubungan, Masyarakat, (, Kadi...</td>\n",
       "      <td>(38, 11, 30, 30, 38, 11, 30, 38, 9, 38, 38, 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16823</th>\n",
       "      <td>(\", Namun, ,, jumlahnya, jauh, lebih, rendah, ...</td>\n",
       "      <td>(38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16824</th>\n",
       "      <td>(\", Dua, kematian, lainnya, melibatkan, pengem...</td>\n",
       "      <td>(38, 0, 38, 38, 38, 38, 14, 38, 38, 38, 38, 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16825</th>\n",
       "      <td>(Dia, mengatakan, tidak, ada, kematian, yang, ...</td>\n",
       "      <td>(38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16826</th>\n",
       "      <td>(Sementara, pada, tahun, 2017, ,, ada, dua, da...</td>\n",
       "      <td>(38, 38, 38, 1, 38, 38, 0, 38, 0, 38, 38, 38, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16827</th>\n",
       "      <td>(Sementara, itu, ,, Thaiveegan, mengatakan, po...</td>\n",
       "      <td>(38, 38, 38, 12, 38, 9, 38, 0, 19, 19, 38, 38,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16828 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  \\\n",
       "0      (Indonesia, mengekspor, produk, industri, skal...   \n",
       "1      (Martahan, Sohuturon, ,, CNN, Indonesia, |, Ra...   \n",
       "2      (Polri, Awasi, Peredaran, Bahan, ', The, Mothe...   \n",
       "3      (Jakarta, ,, CNN, Indonesia, --, Kepolisian, N...   \n",
       "4      (Kepala, Divisi, Hubungan, Masyarakat, (, Kadi...   \n",
       "...                                                  ...   \n",
       "16823  (\", Namun, ,, jumlahnya, jauh, lebih, rendah, ...   \n",
       "16824  (\", Dua, kematian, lainnya, melibatkan, pengem...   \n",
       "16825  (Dia, mengatakan, tidak, ada, kematian, yang, ...   \n",
       "16826  (Sementara, pada, tahun, 2017, ,, ada, dua, da...   \n",
       "16827  (Sementara, itu, ,, Thaiveegan, mengatakan, po...   \n",
       "\n",
       "                                                ner_tags  \n",
       "0      (4, 38, 38, 38, 38, 38, 38, 4, 23, 38, 38, 38,...  \n",
       "1      (12, 31, 38, 11, 30, 38, 1, 20, 20, 20, 20, 20...  \n",
       "2      (9, 38, 38, 38, 38, 14, 33, 33, 33, 38, 9, 38,...  \n",
       "3      (4, 38, 11, 30, 38, 9, 28, 28, 28, 38, 9, 38, ...  \n",
       "4      (38, 11, 30, 30, 38, 11, 30, 38, 9, 38, 38, 38...  \n",
       "...                                                  ...  \n",
       "16823  (38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 0...  \n",
       "16824  (38, 0, 38, 38, 38, 38, 14, 38, 38, 38, 38, 38...  \n",
       "16825  (38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 1...  \n",
       "16826  (38, 38, 38, 1, 38, 38, 0, 38, 0, 38, 38, 38, ...  \n",
       "16827  (38, 38, 38, 12, 38, 9, 38, 0, 19, 19, 38, 38,...  \n",
       "\n",
       "[16828 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hapus data duplikat dataset\n",
    "\n",
    "new_dataset = pd.DataFrame()\n",
    "\n",
    "new_dataset['tokens'] = dataset['tokens'].apply(tuple)\n",
    "new_dataset['ner_tags'] = dataset['ner_tags'].apply(tuple)\n",
    "\n",
    "new_dataset = new_dataset.drop_duplicates()\n",
    "new_dataset = new_dataset.reset_index(drop=True)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5906bf-c85c-4ccb-ba0a-626d71ec87ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Jakarta', '-', 'Gempa', 'bumi', 'berkekuatan', '5', ',', '1', 'skala', 'Richter', 'mengguncang', 'wilayah', 'Klungkung', ',', 'Bali', '.', 'Gempa', 'tersebut', 'berpusat', 'di', 'laut', '.')\n",
      "(4, 38, 38, 38, 38, 15, 34, 34, 34, 34, 38, 38, 7, 38, 4, 38, 38, 38, 38, 38, 38, 38)\n"
     ]
    }
   ],
   "source": [
    "# tokens_cari = ['Gempa', 'bumi', 'mengguncang']\n",
    "# hasil_pencarian = new_dataset[new_dataset['tokens'].apply(lambda x: set(tokens_cari).issubset(x))]\n",
    "# hasil_pencarian\n",
    "\n",
    "print(new_dataset.iloc[136]['tokens'])\n",
    "print(new_dataset.iloc[136]['ner_tags'])\n",
    "\n",
    "# Contoh DataFrame\n",
    "# data = {'Nama': ['John', 'Jane', 'Doe', 'Alice'],\n",
    "#         'Deskripsi': ['Gempa bumi berkekuatan 5,1 skala richer mengguncang wilayah Klungkung',\n",
    "#                       'Cuaca cerah hari ini',\n",
    "#                       'Berita terbaru tentang olahraga',\n",
    "#                       'Klub buku membaca novel bersama']}\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Teks yang ingin dicari\n",
    "# teks_cari = 'Gempa bumi'\n",
    "\n",
    "# # Mencari baris yang mengandung teks tertentu pada sebagian data kolom 'Deskripsi'\n",
    "# hasil_pencarian = df[df['Deskripsi'].str.contains(teks_cari, case=False, regex=True)]\n",
    "\n",
    "# # Menampilkan hasil pencarian\n",
    "# print(\"Hasil Pencarian:\")\n",
    "# print(hasil_pencarian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd601e5e-a2e1-42e2-b3bf-5cceaa5e4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "784633b3-b497-49d3-a957-a2f13bbdf1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13462\n",
      "3366\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = train_test_split(new_dataset, test_size=0.2, random_state=11)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54235b3-896e-4fbb-99e8-1a4a1c448d4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b256c75-5939-431c-979f-2d3557aabc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce5d682-782c-4396-a2bb-03ed0ac5281a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m----> 2\u001b[0m     dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m word_tokenize(sen)\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[1;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[0;32m~/.conda/envs/NER-Project/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_vars\u001b[38;5;241m.\u001b[39mperiod_context_re()\u001b[38;5;241m.\u001b[39mfinditer(text):\n\u001b[1;32m   1396\u001b[0m \n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[1;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "for i, sen in enumerate(dataset['tokens']):\n",
    "    dataset['tokens'][i] = word_tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9efcfc-501c-4051-9367-bc60bfa2061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# digunakan untuk menggabungkan pronouns 's, dolar ($), @, dan hashtag (#)\n",
    "problematic_data_indices = []\n",
    "\n",
    "for i,sen in enumerate(dataset['tokens']):\n",
    "    if len(dataset['tokens'][i]) != len(dataset['ner_tags'][i]):\n",
    "        problematic_data_indices.append(i)\n",
    "\n",
    "for i in problematic_data_indices:\n",
    "    for j, token in enumerate(dataset['tokens'][i]):\n",
    "        if token == \"'s\" or token == \"$\" or token == \"...\" or token == '`' or token == \"'m\" or token == \",\" or token == \".\":\n",
    "            dataset['tokens'][i][j-1] += dataset['tokens'][i][j]\n",
    "            dataset['tokens'][i].pop(j)\n",
    "        \n",
    "        if token == \"#\" or token == \"@\":\n",
    "            dataset['tokens'][i][j] += dataset['tokens'][i][j+1]\n",
    "            dataset['tokens'][i].pop(j+1)\n",
    "            \n",
    "        if (token == \"No\" or token == \"NO\") and dataset['tokens'][i][j+1] == \".\":\n",
    "            dataset['tokens'][i][j] += dataset['tokens'][i][j+1]\n",
    "            dataset['tokens'][i].pop(j+1)\n",
    "            \n",
    "        if token == \"Mr\" and dataset['tokens'][i][j+1] == \".\":\n",
    "            dataset['tokens'][i][j] += dataset['tokens'][i][j+1]\n",
    "            dataset['tokens'][i].pop(j+1)\n",
    "        \n",
    "        if token == \"â€š\" and dataset['tokens'][i][j+1] == \".\":\n",
    "            dataset['tokens'][i][j] += dataset['tokens'][i][j+1]\n",
    "            dataset['tokens'][i].pop(j+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b04b6f-f741-473f-bbb5-004753b11ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58146068-02c4-4f64-9aa7-8a333390e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset['tokens'][0]))\n",
    "# print(len(dataset['ner_tags'][0]))\n",
    "\n",
    "# print(dataset['tokens'][0])\n",
    "# print(nergrit_dataset['train']['tokens'][0])\n",
    "\n",
    "for i,sen in enumerate(dataset['tokens']):\n",
    "    if type(dataset['tokens'][i]) == 'float':\n",
    "        print(i)\n",
    "        break\n",
    "    if len(dataset['tokens'][i]) != len(dataset['ner_tags'][i]):\n",
    "        print(f'not same {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237587f-0053-4e6e-b4c1-1e2c267722d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,token in enumerate(dataset['tokens'][10451]):\n",
    "    if token != nergrit_tokens_combined[10451][i]:\n",
    "        if nergrit_tokens_combined[10451][i] == '-\"': continue\n",
    "        print(f'indes:{i}')\n",
    "        print(dataset['tokens'][10451][i-2:i+3])\n",
    "        print(nergrit_tokens_combined[10451][i-2:i+3])\n",
    "        break\n",
    "        \n",
    "print(dataset['tokens'][10451][:i+3])\n",
    "print(nergrit_tokens_combined[10451][:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73421372-6034-4403-85b1-ac242eae8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_label(dataset,isCount=False,entity_extracted=2):\n",
    "    unique_labels = desc_labels(dataset)\n",
    "    entity_label = {i:0 for i in range(len(unique_labels))} if isCount else {i:[] for i in range(len(unique_labels))}\n",
    "    \n",
    "    dataset = dataset['train']\n",
    "    sentence_data = [seq['tokens'] for seq in dataset]\n",
    "    tokens = [i for seq in dataset for i in seq['tokens']]\n",
    "    labels = [i for seq in dataset for i in seq['ner_tags']]\n",
    "    \n",
    "    # print(tokens)\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if(isCount): entity_label[labels[i]] += 1 \n",
    "        else:\n",
    "            if(len(entity_label[labels[i]])<entity_extracted):\n",
    "                entity_label[labels[i]].append(token)\n",
    "                    \n",
    "    entity_label_key_updated = {unique_labels[i]:entity_label[i] for i in range(len(unique_labels))}\n",
    "    \n",
    "        \n",
    "    return entity_label_key_updated\n",
    "\n",
    "def extract_entity_on_sentence(dataset,sample=2):\n",
    "    unique_labels = desc_labels(dataset)\n",
    "    \n",
    "    dataset = dataset['train']\n",
    "    sentence_data = [seq['tokens'] for seq in dataset]\n",
    "    label_data = [seq['ner_tags'] for seq in dataset]\n",
    "    \n",
    "    label_count = {i:0 for i in range(len(unique_labels))}\n",
    "    output = []\n",
    "    \n",
    "    for i, ner_tags in enumerate(label_data):\n",
    "        if(len(ner_tags)>15): continue\n",
    "        is_save = False\n",
    "        for l in ner_tags:\n",
    "            if(label_count[l]<sample):\n",
    "                label_count[l] += 1\n",
    "                is_save = True\n",
    "        \n",
    "        if(is_save):\n",
    "            output.append([' '.join(sentence_data[i]),[unique_labels[j] for j in label_data[i]]])\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2e56654-ecc9-47e2-9f6f-c9d27dd9d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exxtracted = extract_entity_on_sentence(nergrit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49531344-5667-46b9-9b9a-32ed53b42bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_labels(dataset):\n",
    "    label_names = dataset['train'].info.features['ner_tags'].feature.names\n",
    "    return {i:token for i,token in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "801626d4-c068-4341-87de-5005070948bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(dataset):\n",
    "    info = {\n",
    "        \"label_map\": desc_labels(dataset),\n",
    "        \"samples\": extract_entity_label(nergrit_dataset,entity_extracted=3)\n",
    "    }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8119a51f-f6f8-4a3f-a749-dd71c1b941bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14\n"
     ]
    }
   ],
   "source": [
    "desc_labels(nergrit_dataset)\n",
    "\n",
    "t1 = ('Gempa', 'bumi', 'berkekuatan', '5', ',', '1', 'skala', 'Richter', 'mengguncang', 'wilayah', 'Klungkung', ',', 'Bali', '.')\n",
    "t2 = (38, 38, 38, 15, 34, 34, 34, 34, 38, 38, 7, 38, 4, 38)\n",
    "\n",
    "print(len(t1), len(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76877cc0-3c90-4899-8690-bc06ab493cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-CRD': 3430,\n",
       " 'B-DAT': 3588,\n",
       " 'B-EVT': 1400,\n",
       " 'B-FAC': 481,\n",
       " 'B-GPE': 6451,\n",
       " 'B-LAN': 12,\n",
       " 'B-LAW': 314,\n",
       " 'B-LOC': 2843,\n",
       " 'B-MON': 1292,\n",
       " 'B-NOR': 3893,\n",
       " 'B-ORD': 571,\n",
       " 'B-ORG': 4169,\n",
       " 'B-PER': 6720,\n",
       " 'B-PRC': 746,\n",
       " 'B-PRD': 4142,\n",
       " 'B-QTY': 1509,\n",
       " 'B-REG': 356,\n",
       " 'B-TIM': 811,\n",
       " 'B-WOA': 197,\n",
       " 'I-CRD': 1395,\n",
       " 'I-DAT': 9906,\n",
       " 'I-EVT': 2387,\n",
       " 'I-FAC': 929,\n",
       " 'I-GPE': 2314,\n",
       " 'I-LAN': 11,\n",
       " 'I-LAW': 1350,\n",
       " 'I-LOC': 4529,\n",
       " 'I-MON': 3251,\n",
       " 'I-NOR': 5544,\n",
       " 'I-ORD': 41,\n",
       " 'I-ORG': 5425,\n",
       " 'I-PER': 4357,\n",
       " 'I-PRC': 1472,\n",
       " 'I-PRD': 4182,\n",
       " 'I-QTY': 2446,\n",
       " 'I-REG': 176,\n",
       " 'I-TIM': 2360,\n",
       " 'I-WOA': 368,\n",
       " 'O': 213820}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entity_label(nergrit_dataset,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a03e3e72-2f8d-4024-878b-b86856618371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_map': {0: 'B-CRD',\n",
       "  1: 'B-DAT',\n",
       "  2: 'B-EVT',\n",
       "  3: 'B-FAC',\n",
       "  4: 'B-GPE',\n",
       "  5: 'B-LAN',\n",
       "  6: 'B-LAW',\n",
       "  7: 'B-LOC',\n",
       "  8: 'B-MON',\n",
       "  9: 'B-NOR',\n",
       "  10: 'B-ORD',\n",
       "  11: 'B-ORG',\n",
       "  12: 'B-PER',\n",
       "  13: 'B-PRC',\n",
       "  14: 'B-PRD',\n",
       "  15: 'B-QTY',\n",
       "  16: 'B-REG',\n",
       "  17: 'B-TIM',\n",
       "  18: 'B-WOA',\n",
       "  19: 'I-CRD',\n",
       "  20: 'I-DAT',\n",
       "  21: 'I-EVT',\n",
       "  22: 'I-FAC',\n",
       "  23: 'I-GPE',\n",
       "  24: 'I-LAN',\n",
       "  25: 'I-LAW',\n",
       "  26: 'I-LOC',\n",
       "  27: 'I-MON',\n",
       "  28: 'I-NOR',\n",
       "  29: 'I-ORD',\n",
       "  30: 'I-ORG',\n",
       "  31: 'I-PER',\n",
       "  32: 'I-PRC',\n",
       "  33: 'I-PRD',\n",
       "  34: 'I-QTY',\n",
       "  35: 'I-REG',\n",
       "  36: 'I-TIM',\n",
       "  37: 'I-WOA',\n",
       "  38: 'O'},\n",
       " 'samples': {'B-CRD': ['empat', 'enam', 'satu'],\n",
       "  'B-DAT': ['Selasa', 'Selasa', 'Rabu'],\n",
       "  'B-EVT': ['Final', 'Final', 'final'],\n",
       "  'B-FAC': ['Pelabuhan', 'Pelabuhan', 'Masjid'],\n",
       "  'B-GPE': ['Indonesia', 'Amerika', 'Jakarta'],\n",
       "  'B-LAN': ['bahasa', 'bahasa', 'bahasa'],\n",
       "  'B-LAW': ['surat', 'surat', 'US'],\n",
       "  'B-LOC': ['Jakarta', 'JICT', 'West'],\n",
       "  'B-MON': ['USD', 'Rp', 'Rp'],\n",
       "  'B-NOR': ['Menteri', 'Menteri', 'Menteri'],\n",
       "  'B-ORD': ['Pelindo', 'pertama', 'pertama'],\n",
       "  'B-ORG': ['IPC', 'CNN', 'Baintelkam'],\n",
       "  'B-PER': ['Presiden', 'Presiden', 'Budi'],\n",
       "  'B-PRC': ['50', '15', '10'],\n",
       "  'B-PRD': ['sepatu', 'garmen', 'karet'],\n",
       "  'B-QTY': ['10', '95.263', 'Gross'],\n",
       "  'B-REG': ['Ramadan', 'Ramadan', 'bulan'],\n",
       "  'B-TIM': ['08', '07', '07'],\n",
       "  'B-WOA': ['A', 'The', 'Patung'],\n",
       "  'I-CRD': ['Juta', 'Juta', 'Juta'],\n",
       "  'I-DAT': ['15', '8', '2018'],\n",
       "  'I-EVT': ['Piala', 'FA', '2018'],\n",
       "  'I-FAC': ['Tanjung', 'Priok', 'Tanjung'],\n",
       "  'I-GPE': ['Serikat', 'Utara', 'Serikat'],\n",
       "  'I-LAN': ['Indonesia', 'Rusia', 'Indonesia'],\n",
       "  'I-LAW': ['keputusan', 'Kapolri', 'utang'],\n",
       "  'I-LOC': ['International', 'Container', 'Terminal'],\n",
       "  'I-MON': ['300', '3', 'M'],\n",
       "  'I-NOR': ['Perhubungan', 'Perindustrian', 'Perdagangan'],\n",
       "  'I-ORD': ['bulan', 'pertama', '2018'],\n",
       "  'I-ORG': ['Indonesia', 'Humas', 'Polri'],\n",
       "  'I-PER': ['Karya', 'Sumadi', 'Hartarto'],\n",
       "  'I-PRC': ['persen', 'persen', 'persen'],\n",
       "  'I-PRD': ['MV', 'CMA', 'CGM'],\n",
       "  'I-QTY': ['ribu', 'TEUs', 'GT'],\n",
       "  'I-REG': ['suci', 'Ramadan', 'suci'],\n",
       "  'I-TIM': [':', '20', 'WIB'],\n",
       "  'I-WOA': ['Trip', 'to', 'the'],\n",
       "  'O': ['mengekspor', 'produk', 'industri']}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_info(nergrit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f77db-8f72-47ae-b756-f9e1f64dbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nergrit_dataset['train']['tokens'][140]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce0f1e-1ecf-4d13-9d80-5e621a834e7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FASTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7964068c-7b23-4acd-b2ab-892737bd84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bd17d02-9de7-4232-ba91-47ca6b12f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "id_fasttext = fasttext.load_model('./cc.id.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "346461be-ff08-4136-87ae-16ae2f612c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6171797513961792, 'irnya'),\n",
       " (0.5662643909454346, 'patotoe'),\n",
       " (0.5593110918998718, 'ir-sya'),\n",
       " (0.5553608536720276, 'Sya'),\n",
       " (0.5534719824790955, ',,saya'),\n",
       " (0.5462336540222168, 'Patotoe'),\n",
       " (0.5404911637306213, ',saya'),\n",
       " (0.5337669253349304, 'seblmnya'),\n",
       " (0.5295220613479614, 'riat'),\n",
       " (0.5264296531677246, 'riah')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_fasttext.get_nearest_neighbors('sya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309e32e-eff4-4948-9547-5ec711356f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'origin dimension: {id_fasttext.get_dimension()}')\n",
    "\n",
    "fasttext.util.reduce_model(id_fasttext, 2)\n",
    "\n",
    "print(f'reduced dimension: {id_fasttext.get_dimension()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4db76a20-c06a-4d14-b910-bcd919b01740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24763355 0.04528186]\n",
      "[0.15636204 0.02645837]\n",
      "[0.29079488 0.04939094]\n",
      "[0.09388968 0.46004158]\n",
      "[-0.04135711 -0.00143757]\n",
      "[0.39425156 0.28984338]\n",
      "[0.3626357  0.05012034]\n",
      "[0.13619876 0.3875976 ]\n",
      "[-0.13530862  0.7744079 ]\n",
      "[0.45093042 0.14592652]\n",
      "[-0.01554188  0.40266582]\n",
      "[0.00469462 0.09509537]\n",
      "[0.03605656 0.12732781]\n",
      "[0.28490165 0.06851771]\n",
      "[0.21356207 0.00209849]\n",
      "[0.11995699 0.01652862]\n",
      "[0.46251014 0.05710749]\n"
     ]
    }
   ],
   "source": [
    "tes = ['sudah', 'dicoba', 'dan', 'tet', '##ep', 'gak', 'bisa', 'mba', ',', 'ada', 'not', '##if', '##2', 'tidak', 'jelas', 'seperti', 'itu']\n",
    "\n",
    "for i in tes:\n",
    "    print(id_fasttext[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0251db2-07c0-4a98-b854-3510f58458f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00320197,  0.04431295,  0.00596271,  0.0003198 , -0.00154018,\n",
       "       -0.00504316,  0.01854377,  0.00140952,  0.00416551, -0.0064503 ,\n",
       "        0.0017719 , -0.01546858, -0.00787772,  0.01578196, -0.0111641 ,\n",
       "        0.02190386, -0.00518977, -0.00415276, -0.02037244, -0.01143345,\n",
       "        0.00754148,  0.00148768, -0.00179641, -0.01104866,  0.01832668,\n",
       "       -0.00375121,  0.00497834, -0.0064133 , -0.02198054,  0.0020031 ,\n",
       "        0.00191203, -0.00624121,  0.00103681, -0.00464204,  0.00391369,\n",
       "       -0.01440342,  0.01218161,  0.01042238, -0.00031999,  0.0007526 ,\n",
       "        0.00047804,  0.01641218, -0.01223699,  0.00625934,  0.0068442 ,\n",
       "       -0.00454213,  0.01949731,  0.002113  ,  0.00531257, -0.0149791 ,\n",
       "        0.00276029,  0.02538211, -0.00658982,  0.01531404, -0.01573758,\n",
       "       -0.02086379,  0.01706518,  0.01133318, -0.00160001, -0.01392083,\n",
       "        0.00370123,  0.00080782,  0.00645986, -0.00170984,  0.00422715,\n",
       "        0.00382178,  0.00219005, -0.00318533, -0.00171774,  0.00760908,\n",
       "        0.00700776, -0.00663567,  0.00196944, -0.01442075,  0.0017066 ,\n",
       "        0.00864465,  0.0061098 ,  0.0060526 ,  0.00029894, -0.00370024,\n",
       "        0.01111616, -0.01685457, -0.01902913,  0.01048322,  0.01709114,\n",
       "        0.00101889, -0.0041495 , -0.01284732, -0.0126268 ,  0.00177523,\n",
       "       -0.00028245, -0.00033928, -0.01333768,  0.00569926, -0.00923727,\n",
       "        0.00591878,  0.01304101,  0.01161189, -0.00898187,  0.0069406 ,\n",
       "       -0.00844761,  0.01018663, -0.00354307,  0.00688495, -0.0100689 ,\n",
       "       -0.00507168, -0.00278535, -0.00089352,  0.02242895,  0.00700844,\n",
       "        0.00318853, -0.02111614, -0.00583677, -0.0222303 , -0.01171141,\n",
       "       -0.00516171,  0.00427773, -0.00984897, -0.0038085 ,  0.00823074,\n",
       "        0.00806255,  0.00282799, -0.00703303,  0.00258375, -0.01602085,\n",
       "        0.00062712,  0.00510873, -0.00130482, -0.00193779, -0.01156048,\n",
       "       -0.00842386,  0.00346926, -0.00754634,  0.00499881,  0.00197424,\n",
       "        0.00665699,  0.00772995,  0.00108247,  0.01043626,  0.00193189,\n",
       "        0.00112679,  0.0061601 , -0.00195984, -0.01178111, -0.00911605,\n",
       "       -0.00398548, -0.00716327,  0.02275079, -0.00765215, -0.00019349,\n",
       "       -0.00485381, -0.00670538,  0.00805026,  0.00987896,  0.01280174,\n",
       "       -0.00266761,  0.01361457,  0.00102048,  0.00021827, -0.00015582,\n",
       "        0.00104362, -0.00074215,  0.0042658 , -0.00058799, -0.00595411,\n",
       "        0.002487  ,  0.01519306,  0.00281309,  0.00131221,  0.00356822,\n",
       "        0.00134191, -0.01020435, -0.0116003 , -0.00148608,  0.01053818,\n",
       "       -0.00368755,  0.00192966,  0.00078045, -0.00801874,  0.00412811,\n",
       "        0.00336639, -0.00082989,  0.00351989, -0.00689254,  0.00480074,\n",
       "        0.00370705, -0.01232871,  0.01050726, -0.00011134, -0.00784695,\n",
       "        0.00215559,  0.00134386,  0.00091953,  0.00280573, -0.00502391,\n",
       "       -0.00011244, -0.00050778,  0.00765846,  0.00109525,  0.00181126],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_fasttext['jdhersomvf']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel1",
   "language": "python",
   "name": "kernel1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
